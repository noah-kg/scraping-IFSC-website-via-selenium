{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa1373f-a803-4146-b30b-12291b14fd46",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "## Import Libraries & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f437b8d-efdc-4953-902f-6efb6160713f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T21:32:59.497594Z",
     "iopub.status.busy": "2023-05-22T21:32:59.496617Z",
     "iopub.status.idle": "2023-05-22T21:32:59.576177Z",
     "shell.execute_reply": "2023-05-22T21:32:59.576177Z",
     "shell.execute_reply.started": "2023-05-22T21:32:59.497594Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d17bc8-1883-4d02-b297-ad9abb029de3",
   "metadata": {},
   "source": [
    "## Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92492156-0021-4d11-b87a-2ad2f2e193cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T21:32:59.577144Z",
     "iopub.status.busy": "2023-05-22T21:32:59.577144Z",
     "iopub.status.idle": "2023-05-22T21:32:59.583977Z",
     "shell.execute_reply": "2023-05-22T21:32:59.583977Z",
     "shell.execute_reply.started": "2023-05-22T21:32:59.577144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\n",
    "    os.path.dirname(os.path.realpath(\"__file__\")), \"data\"\n",
    ")\n",
    "\n",
    "BOULDER_MEN_DIR            = os.path.join(DATA_DIR, \"Boulder/Men\")\n",
    "BOULDER_WOMEN_DIR          = os.path.join(DATA_DIR, \"Boulder/Women\")\n",
    "LEAD_MEN_DIR               = os.path.join(DATA_DIR, \"Lead/Men\")\n",
    "LEAD_WOMEN_DIR             = os.path.join(DATA_DIR, \"Lead/Women\")\n",
    "SPEED_MEN_DIR              = os.path.join(DATA_DIR, \"Speed/Men\")\n",
    "SPEED_WOMEN_DIR            = os.path.join(DATA_DIR, \"Speed/Women\")\n",
    "COMBINED_MEN_DIR           = os.path.join(DATA_DIR, \"Combined/Men\")\n",
    "COMBINED_WOMEN_DIR         = os.path.join(DATA_DIR, \"Combined/Women\")\n",
    "BOULDER_AND_LEAD_MEN_DIR   = os.path.join(DATA_DIR, \"Boulder & Lead/Men\")\n",
    "BOULDER_AND_LEAD_WOMEN_DIR = os.path.join(DATA_DIR, \"Boulder & Lead/Women\")\n",
    "\n",
    "dirs = [BOULDER_MEN_DIR, BOULDER_WOMEN_DIR, LEAD_MEN_DIR, LEAD_WOMEN_DIR,\n",
    "       SPEED_MEN_DIR, SPEED_WOMEN_DIR, COMBINED_MEN_DIR, COMBINED_WOMEN_DIR,\n",
    "       BOULDER_AND_LEAD_MEN_DIR, BOULDER_AND_LEAD_WOMEN_DIR]\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "for dir in dirs:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "# File to store names of events that have already been scraped\n",
    "try:\n",
    "    ALREADY_SCRAPED = os.path.join(DATA_DIR, \"scraped_events.txt\")\n",
    "    # Create file\n",
    "    with open(ALREADY_SCRAPED, 'x') as fp:\n",
    "        pass\n",
    "except:\n",
    "    if os.stat(ALREADY_SCRAPED).st_size == 0:\n",
    "        print(\"No data has been scraped yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2d3c4-a7e4-4804-87c2-8ed71e1df696",
   "metadata": {},
   "source": [
    "## IFSCScraper Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b766d9d-36c2-41b9-acb0-7a413519ba06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T21:32:59.584952Z",
     "iopub.status.busy": "2023-05-22T21:32:59.584952Z",
     "iopub.status.idle": "2023-05-22T21:32:59.612329Z",
     "shell.execute_reply": "2023-05-22T21:32:59.612329Z",
     "shell.execute_reply.started": "2023-05-22T21:32:59.584952Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IFSCScraper():\n",
    "    \"\"\"\n",
    "    Define a class for the scraper that will be used to gather data from the IFSC website\n",
    "    (ifsc-climbing.org). Includes methods that allow for scraping different pages and \n",
    "    different information.\n",
    "    \"\"\"\n",
    "    # Page url\n",
    "    url  = 'https://www.ifsc-climbing.org/index.php/world-competition/last-result'\n",
    "    url2 = 'https://ifsc.results.info/#/athlete/'\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a scraper object with its own browser instance.\n",
    "        \n",
    "        debug: indicates whether this is a debug instance for quicker development\n",
    "        \"\"\"\n",
    "        self.generate_driver()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    def generate_driver(self):\n",
    "        \"\"\"\n",
    "        Initialize Selenium web browser.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sets headless option, so we don't see browser\n",
    "            headOption = webdriver.FirefoxOptions()\n",
    "            headOption.add_argument(\"--headless\")\n",
    "            self.driver = webdriver.Firefox(options=headOption)\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "        except Exception as ex:\n",
    "            template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "            message = template.format(type(ex).__name__, ex.args)\n",
    "            print(message)\n",
    "        \n",
    "    def load_page(self, link, athlete_page=0, timeout=10, wait_after=1):\n",
    "        \"\"\"\n",
    "        Helper function that opens browser, loads a page, and waits for timeout.\n",
    "        \n",
    "        link: link to the page we wish to load\n",
    "        athlete_page: flag to load different url\n",
    "        timeout: seconds to wait before timing out\n",
    "        wait_after: seconds to wait after loading\n",
    "        \"\"\"\n",
    "\n",
    "        # Visit link\n",
    "        self.driver.get(link)\n",
    "        wait = WebDriverWait(self.driver, timeout)\n",
    "\n",
    "        # Attempt to open link\n",
    "        try:\n",
    "            if athlete_page:\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='athlete-info left-side']\")))\n",
    "            else:\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='uk-container']\")))\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page \" + link + \" to load\")\n",
    "            self.driver.quit()\n",
    "\n",
    "        # Wait for page to load\n",
    "        time.sleep(wait_after)\n",
    "    \n",
    "    def get_year_list(self):\n",
    "        \"\"\"\n",
    "        Opens browser and gets list of all years listed on IFSC website.\n",
    "        \n",
    "        Returns list of years (as strings)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.load_page(IFSCScraper.url)            \n",
    "            wait = WebDriverWait(self.driver, 5)\n",
    "            \n",
    "            # The data we are after resides within an iframe\n",
    "            wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe.jch-lazyloaded\")))\n",
    "        except:\n",
    "            print('Error loading page!')\n",
    "            \n",
    "        # Dropdown menus for each choice\n",
    "        year_dd, league_dd, event_dd, cat_dd = self.get_dropdowns(self.driver)\n",
    "        year_opts = Select(year_dd).options\n",
    "        return [year.text for year in year_opts]\n",
    "    \n",
    "    def get_single_year(self, year='2022'):\n",
    "        \"\"\"\n",
    "        Fully scrape each event and category for a given year.\n",
    "        \n",
    "        year: string of the year you want to scrape\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.load_page(IFSCScraper.url)            \n",
    "            wait = WebDriverWait(self.driver, 8)\n",
    "            \n",
    "            # The data we are after resides within an iframe\n",
    "            wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe.jch-lazyloaded\")))\n",
    "            print(f'Scraping {year}...')\n",
    "        except:\n",
    "            print('Error loading page!')        \n",
    "        \n",
    "        # Dropdown menus for each choice\n",
    "        year_dd, league_dd, event_dd, cat_dd = self.get_dropdowns(self.driver)\n",
    "        \n",
    "        # Select given year and league\n",
    "        year_ob   = Select(year_dd).select_by_visible_text(year)\n",
    "        league_ob = Select(league_dd).select_by_index(1)\n",
    "        \n",
    "        # Get list of all events for the year\n",
    "        all_events = self.get_events(self.driver, event_dd)\n",
    "        \n",
    "        # Loop through each event, scrape results, and generate .csv file\n",
    "        dfs = []\n",
    "        for i, event in enumerate(all_events):\n",
    "            # Implement check to see if event has already been scraped\n",
    "            if self.check_if_scraped(event) and all_events[i] != all_events[i-1]:\n",
    "                print(f'--Already scraped {event}!')\n",
    "                continue\n",
    "            # Some of the events aren't actually events, but more qualification rounds, and \n",
    "            # they don't list the results correctly, which will cause errors. The common thread\n",
    "            # is the naming of them.\n",
    "            elif event.count('(') < 1:\n",
    "                print(f'--Skipping {event}...')\n",
    "                continue\n",
    "            else:\n",
    "                # Set this flag for special cases where two events share the same name (rare)\n",
    "                same_event_name = True if all_events[i] == all_events[i-1] else False\n",
    "                                    \n",
    "                # Select event\n",
    "                if same_event_name:\n",
    "                    event_ob = Select(event_dd).select_by_index(i+1)                    \n",
    "                else:\n",
    "                    event_ob = Select(event_dd).select_by_visible_text(event)\n",
    "                    \n",
    "                category_select = Select(cat_dd)\n",
    "\n",
    "                # Some events were cancelled or don't have results listed, check for it here\n",
    "                try:\n",
    "                    wait.until(lambda d: len(category_select.options) > 1)\n",
    "                    if not self.check_if_scraped(event):\n",
    "                        # self.add_to_scraped_file(event)\n",
    "                        print(f'--Scraping {event}...')\n",
    "                except:\n",
    "                    print(f'--No data for {event}!')\n",
    "                    continue\n",
    "\n",
    "                # Get results for each category\n",
    "                for cat in category_select.options[1:]:\n",
    "                    cat_ob = Select(cat_dd).select_by_visible_text(cat.text) # selects category\n",
    "\n",
    "                    # Finds table with desired data\n",
    "                    try:\n",
    "                        wait.until(EC.visibility_of_element_located((By.XPATH, '//div[@id=\"table_id_wrapper\"]')))\n",
    "                    except:\n",
    "                        print(f'----No data for {cat.text}!')\n",
    "                        continue\n",
    "\n",
    "                    table_wrapper = self.driver.find_element(By.XPATH, '//div[@id=\"table_id_wrapper\"]')\n",
    "                    results = table_wrapper.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "                    # Get event name and date\n",
    "                    event_details = self.driver.find_element(By.XPATH, '//div[@class=\"labels\"]')\n",
    "                    event_results = event_details.find_elements(By.TAG_NAME, 'p') # Event title & date\n",
    "\n",
    "                    # Sets event name in case of duplicate\n",
    "                    if same_event_name:\n",
    "                        event = event[:-10] + '2 ' + event[-10:]\n",
    "                        same_event_name = False\n",
    "                    \n",
    "                    # Generate correct filename\n",
    "                    file = self.generate_filename((cat.text, event_results[0].text, event_results[1].text))\n",
    "                    text = '--' + file\n",
    "                    path = self.get_dir(cat.text)                    \n",
    "                    filepath = os.path.join(path, file)\n",
    "                                        \n",
    "                    # Checks if the filename has been added to the .txt, AND if the file exists\n",
    "                    if self.check_if_scraped(text, filepath):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f'----Scraping {cat.text}...')\n",
    "                        \n",
    "                        # Data (list of dictionaries) contains each climber's results\n",
    "                        data = []\n",
    "                        for result in results:\n",
    "                            # Each climber's result stored in dict\n",
    "                            temp_dict = self.scrape_results(event, result, cat.text)\n",
    "                            \n",
    "                            if temp_dict:\n",
    "                                data.append(temp_dict)\n",
    "                            else:\n",
    "                                print(f'----Data format error for {cat.text}! Cannot parse it further. Skipping...')\n",
    "                                break\n",
    "                        \n",
    "                        if data:\n",
    "                            # Convert raw results into a .csv and marks file as scraped\n",
    "                            df = pd.DataFrame.from_dict(data)                            \n",
    "                            self.convert_to_csv(file, cat.text, df)\n",
    "                            self.add_to_scraped_file('--' + file)\n",
    "                        \n",
    "                # All categories for the event have been scraped\n",
    "                self.add_to_scraped_file(event)\n",
    "                        \n",
    "    def get_athlete_height(self, athlete_id):\n",
    "        \"\"\"\n",
    "        Scrape athlete page for climber's height\n",
    "        \n",
    "        athlete_id: unique id assigned to each climber\n",
    "        Returns height of given climber\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.load_page(IFSCScraper.url2 + athlete_id, athlete_page=1)            \n",
    "        except:\n",
    "            print('Error loading page!')\n",
    "            self.driver.quit()\n",
    "        \n",
    "        height_div = self.driver.find_element(By.XPATH, '//div[@class=\"athlete-info left-side\"]')\n",
    "        height_ele = height_div.find_elements(By.TAG_NAME, 'div')[1]\n",
    "        return height_ele.text.split(': ')[1]\n",
    "    \n",
    "    def add_to_scraped_file(self, text):\n",
    "        \"\"\"\n",
    "        Adds given text to our tracker file to keep track of what has been scraped.\n",
    "        \n",
    "        text: string of text to add to file\n",
    "        \"\"\"\n",
    "        if not self.check_if_scraped(text):\n",
    "            with open(ALREADY_SCRAPED, 'a') as file:\n",
    "                file.write(f'{text}\\n')\n",
    "                return\n",
    "    \n",
    "    def check_if_scraped(self, text, file = ''):\n",
    "        \"\"\"\n",
    "        Checks if the given text exists in the given file.\n",
    "        \n",
    "        text: string of text to check in file\n",
    "        file: name of file to check for existence\n",
    "        Returns true/false\n",
    "        \"\"\"\n",
    "        with open(ALREADY_SCRAPED, 'r') as f:\n",
    "            done = [x.strip() for x in f.readlines()]\n",
    "            \n",
    "        if file:\n",
    "            if text in done and os.path.exists(file):\n",
    "                return True\n",
    "            return False\n",
    "        return text in done\n",
    "\n",
    "    def scrape_results(self, event, result, cat):\n",
    "        \"\"\"\n",
    "        Function to scrape the desired results from a given event and category.\n",
    "        \n",
    "        event: name of event to scrape\n",
    "        result: individual climber's results for given event\n",
    "        cat: category (men's or women's)\n",
    "        Returns dictionary with correct result format based on category\n",
    "        \"\"\"\n",
    "        details = result.find_elements(By.TAG_NAME, 'td')\n",
    "        athlete_id = details[1].find_element(By.TAG_NAME, 'a').get_attribute('href').split('id=')[1]\n",
    "\n",
    "        if \"LEAD\" in cat or \"BOULDER\" in cat:\n",
    "            try:                \n",
    "                temp_dict = {\n",
    "                    \"Event\": event,\n",
    "                    \"ID\": athlete_id,\n",
    "                    \"Rank\": details[0].text,\n",
    "                    \"Name\": f\"{details[1].text} {details[2].text}\",\n",
    "                    \"Gender\": 'F' if 'WOMEN' in cat.upper() else 'M',\n",
    "                    \"Country\": details[3].text,\n",
    "                    \"Qualification\": details[4].text,\n",
    "                    \"Semi-Final\": details[5].text\n",
    "                }\n",
    "                if len(details) == 7: #each round has data\n",
    "                    temp_dict[\"Final\"] = details[6].text\n",
    "                else: # semi-final round acts as final\n",
    "                    temp_dict[\"Final\"] = details[5].text\n",
    "                \n",
    "            except:\n",
    "                return False\n",
    "        elif \"SPEED\" in cat:\n",
    "            try:\n",
    "                temp_dict = {\n",
    "                    \"Event\": event,\n",
    "                    \"ID\": athlete_id,\n",
    "                    \"Rank\": details[0].text,\n",
    "                    \"Name\": f\"{details[1].text} {details[2].text}\",\n",
    "                    \"Gender\": 'F' if 'WOMEN' in cat.upper() else 'M',\n",
    "                    \"Country\": details[3].text,\n",
    "                    \"Qualification\": details[4].text,\n",
    "                    \"Final\": details[5].text\n",
    "                }\n",
    "            except:\n",
    "                return False\n",
    "        else:\n",
    "            try:\n",
    "                temp_dict = {\n",
    "                    \"Event\": event,\n",
    "                    \"ID\": athlete_id,\n",
    "                    \"Rank\": details[0].text,\n",
    "                    \"Name\": f\"{details[1].text} {details[2].text}\",\n",
    "                    \"Gender\": 'F' if 'WOMEN' in cat.upper() else 'M',\n",
    "                    \"Country\": details[3].text,\n",
    "                    \"Qualification\": details[4].text\n",
    "                }\n",
    "            except:\n",
    "                return False\n",
    "        return temp_dict\n",
    "    \n",
    "    def get_dropdowns(self, driver):\n",
    "        \"\"\"\n",
    "        Helper function to quickly find the four dropdown menus on the page.\n",
    "        \n",
    "        driver: selenium web driver\n",
    "        Returns one web element for each dropdown menu\n",
    "        \"\"\"\n",
    "        year_dd   = driver.find_element(By.XPATH, '//select[@id=\"years\"]')\n",
    "        league_dd = driver.find_element(By.XPATH, '//select[@id=\"indexes\"]')\n",
    "        event_dd  = driver.find_element(By.XPATH, '//select[@id=\"events\"]')\n",
    "        cat_dd    = driver.find_element(By.XPATH, '//select[@id=\"categories\"]')        \n",
    "        return year_dd, league_dd, event_dd, cat_dd\n",
    "    \n",
    "    def get_events(self, driver, events_dd):\n",
    "        \"\"\"\n",
    "        Function to get a list of all events.\n",
    "        \n",
    "        driver: selenium web element\n",
    "        events_dd: dropdown element for the event menu\n",
    "        Returns list of events in the events_dd menu\n",
    "        \"\"\"\n",
    "        event_opts = Select(events_dd)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(lambda d: len(event_opts.options) > 1)                    \n",
    "        return [x.text for x in event_opts.options[1:]]\n",
    "    \n",
    "    def generate_filename(self, packed_data):\n",
    "        \"\"\"\n",
    "        Function to take the event name and convert it to a consistent, formatted filename.\n",
    "        \n",
    "        packed_data: tuple containing all necessary info to generate the filename\n",
    "        \"\"\"\n",
    "        # Unpacks data\n",
    "        (category, event, date) = packed_data\n",
    "\n",
    "        # Create filename in form of {date}_{event}_{category}\n",
    "        date = ' '.join(date.split()[::-1][:3])       \n",
    "\n",
    "        # Cleans up event name for next part\n",
    "        event = event.replace('- ', '').split()\n",
    "        if event[-1] == 'CANCELLED':\n",
    "            event = ' '.join(event[:-2])\n",
    "        else:\n",
    "            event = ' '.join(event[:-1])\n",
    "\n",
    "        # Uses Regex to clean because not every name has the same format\n",
    "        filename = ' '.join([date, event, category])\n",
    "        filename = re.findall(\"^[^\\(]+|[\\(].*\", filename)\n",
    "        filename[1] = filename[1].split(') ', 1)[1]\n",
    "        filename = (''.join(filename)\n",
    "                    .replace('(','[')\n",
    "                    .replace(')',']')\n",
    "                    .replace(' ', '_')\n",
    "                    .replace(',', '')\n",
    "                    .lower()) + '.csv'        \n",
    "        return filename\n",
    "\n",
    "    def convert_to_csv(self, filename, category, data):\n",
    "        \"\"\"\n",
    "        Function to take the given data and create a .csv with the given filename\n",
    "        in the given category directory.\n",
    "        \n",
    "        filename: filename generated by generate_filename function\n",
    "        category: men's or women's (used to differentiate directory)\n",
    "        data: dataframe we want to save\n",
    "        \"\"\"\n",
    "        # Figure out correct directory\n",
    "        path = self.get_dir(category)\n",
    "        file = path + f'\\\\{filename}'\n",
    "\n",
    "        # Generates .csv with filename\n",
    "        data.to_csv(file, index=False)\n",
    "            \n",
    "    def get_dir(self, category):\n",
    "        \"\"\"\n",
    "        Function to return the proper directory to use, based on category.\n",
    "        \n",
    "        category: men's or women's\n",
    "        \"\"\"\n",
    "        base = category.upper().split()\n",
    "        if \"MEN\" in base:\n",
    "            if \"BOULDER\" in base: return BOULDER_MEN_DIR\n",
    "            if \"LEAD\" in base: return LEAD_MEN_DIR\n",
    "            if \"SPEED\" in base: return SPEED_MEN_DIR\n",
    "            if \"COMBINED\" in base: return COMBINED_MEN_DIR\n",
    "            if \"BOULDER&LEAD\" in base: return BOULDER_AND_LEAD_MEN_DIR\n",
    "        if \"WOMEN\" in base:\n",
    "            if \"BOULDER\" in base: return BOULDER_WOMEN_DIR\n",
    "            if \"LEAD\" in base: return LEAD_WOMEN_DIR\n",
    "            if \"SPEED\" in base: return SPEED_WOMEN_DIR\n",
    "            if \"COMBINED\" in base: return COMBINED_WOMEN_DIR\n",
    "            if \"BOULDER&LEAD\" in base: return BOULDER_AND_LEAD_WOMEN_DIR\n",
    "                \n",
    "    def end_session(self):\n",
    "        \"\"\"Basic function to end the selenium web driver and close the browser.\"\"\"\n",
    "        print('SESSION DONE! Quitting webdriver and closing browser...')\n",
    "        self.driver.quit()\n",
    "        \n",
    "    def scrape_all_ifsc_world_cups(self):\n",
    "        \"\"\"\n",
    "        Complete function to go through each year and scrape all IFSC World Cup\n",
    "        competitions. Currently does not include 2023 because those events have\n",
    "        not happened yet.\n",
    "        \"\"\"\n",
    "        years = self.get_year_list()\n",
    "        for year in years[:-17]: # 2023-2007, 2023 events ongoing\n",
    "            scraper.get_single_year(year)\n",
    "        self.end_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e6fe95-2e7a-4fa5-9093-6f2d6fab89ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T21:32:59.613304Z",
     "iopub.status.busy": "2023-05-22T21:32:59.613304Z",
     "iopub.status.idle": "2023-05-22T21:34:34.043970Z",
     "shell.execute_reply": "2023-05-22T21:34:34.043970Z",
     "shell.execute_reply.started": "2023-05-22T21:32:59.613304Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 2023...\n",
      "--Already scraped IFSC - Climbing World Cup (B) - Hachioji (JPN) 2023!\n",
      "--Already scraped IFSC - Climbing World Cup (B,S) - Seoul (KOR) 2023!\n",
      "--Already scraped IFSC - Climbing World Cup (S) - Jakarta (INA) 2023!\n",
      "--Scraping IFSC - Climbing World Cup (B,S) - Salt Lake City (USA) 2023...\n",
      "----Scraping BOULDER Men...\n",
      "----Scraping BOULDER Women...\n",
      "----Scraping SPEED Men...\n",
      "----Scraping SPEED Women...\n",
      "--No data for IFSC - Climbing World Cup (B) - Prague (CZE) 2023!\n",
      "--No data for IFSC - Climbing World Cup (B) - Brixen (ITA) 2023!\n",
      "--No data for IFSC - Climbing World Cup (B,L) - Innsbruck (AUT) 2023!\n",
      "--No data for IFSC - Climbing World Cup (L,S) - Villars (SUI) 2023!\n",
      "--No data for IFSC - Climbing World Cup (L,S) - Chamonix (FRA) 2023!\n",
      "--No data for IFSC - Climbing World Cup (L) - Briançon (FRA) 2023!\n",
      "--No data for IFSC - Climbing World Championships (B,L,S,B&L) - Bern (SUI) 2023!\n",
      "--No data for IFSC - Climbing World Cup (L) - Koper (SLO) 2023!\n",
      "--No data for IFSC - Climbing World Cup (L,S) - Wujiang (CHN) 2023!\n",
      "SESSION DONE! Quitting webdriver and closing browser...\n"
     ]
    }
   ],
   "source": [
    "# Takes ~37 minutes to scrape all years and events\n",
    "scraper = IFSCScraper()\n",
    "# scraper.scrape_all_ifsc_world_cups() # Run once to gather everything\n",
    "scraper.get_single_year(scraper.get_year_list()[0]) # Gets results for most recent year\n",
    "scraper.end_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
